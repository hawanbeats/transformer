# âš™ï¸ Basit Transformer Encoder UygulamasÄ± (NumPy ile)

Bu proje, **Transformer Encoder** bloÄŸunun NumPy ile sÄ±fÄ±rdan, sade bir ÅŸekilde nasÄ±l uygulanabileceÄŸini gÃ¶sterir. Temel olarak **multi-head self-attention**, **layer normalization** ve **feed-forward aÄŸlar** gibi bileÅŸenleri iÃ§erir.

## ğŸ“Œ Ä°Ã§erik

- Softmax ve Layer Normalization
- Multi-Head Self-Attention MekanizmasÄ±
- Basit Fully Connected Feed-Forward KatmanÄ±
- Residual Connection + Layer Normalization
- Heatmap ile dikkat (attention) gÃ¶rselleÅŸtirmesi

## ğŸ§  Transformer Encoder YapÄ±sÄ±

Kod aÅŸaÄŸÄ±daki iÅŸlem sÄ±rasÄ±nÄ± takip eder:

1. **Multi-Head Attention** uygulanÄ±r.
2. Elde edilen Ã§Ä±ktÄ±lar giriÅŸ ile toplanÄ±p **Layer Normalization** uygulanÄ±r.
3. **Feed-Forward Neural Network** uygulanÄ±r.
4. Tekrar residual + **Layer Normalization** yapÄ±lÄ±r.
5. Final Ã§Ä±ktÄ± ve attention aÄŸÄ±rlÄ±klarÄ± dÃ¶ndÃ¼rÃ¼lÃ¼r.

## ğŸ“Š GÃ¶rselleÅŸtirme

AÅŸaÄŸÄ±da, bir attention head iÃ§in dikkat aÄŸÄ±rlÄ±k matrisi Ã¶rneÄŸi gÃ¶sterilmektedir:

![Attention Head 1](./images/attention_head1.png)

## ğŸ› ï¸ Kurulum

AÅŸaÄŸÄ±daki kÃ¼tÃ¼phaneler gereklidir:

```bash
pip install numpy matplotlib seaborn
```
## ğŸ” Notlar
Bu Ã¶rnek eÄŸitim amaÃ§lÄ±dÄ±r.

BÃ¼yÃ¼k dil modellerindeki karmaÅŸÄ±k yapÄ±lar sadeleÅŸtirilmiÅŸtir.

Kod, giriÅŸ olarak rastgele 4x8 boyutlu vektÃ¶rler ile Ã§alÄ±ÅŸÄ±r.

GerÃ§ek modellerde aÄŸÄ±rlÄ±k matrisleri Ã¶ÄŸrenilir; burada ise rastgele baÅŸlatÄ±lÄ±r.
